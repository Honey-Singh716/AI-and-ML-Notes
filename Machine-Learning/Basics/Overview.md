1) Definition / derivation (scalar and vector forms)

For a dataset with 
ğ‘š
m examples, true labels 
ğ‘¦
(
ğ‘–
)
y
(i)
 and predictions 
ğ‘¦
^
(
ğ‘–
)
y
^
	â€‹

(i)
:

Mean Squared Error (MSE):

MSE
â€…â€Š
=
â€…â€Š
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
(
ğ‘¦
^
(
ğ‘–
)
âˆ’
ğ‘¦
(
ğ‘–
)
)
2
MSE=
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

(
y
^
	â€‹

(i)
âˆ’y
(i)
)
2

Root Mean Squared Error (RMSE):

RMSE
â€…â€Š
=
â€…â€Š
MSE
â€…â€Š
=
â€…â€Š
1
ğ‘š
âˆ‘
ğ‘–
=
1
ğ‘š
(
ğ‘¦
^
(
ğ‘–
)
âˆ’
ğ‘¦
(
ğ‘–
)
)
2
RMSE=
MSE
	â€‹

=
m
1
	â€‹

i=1
âˆ‘
m
	â€‹

(
y
^
	â€‹

(i)
âˆ’y
(i)
)
2
	â€‹


If your model is linear with parameters 
ğœƒ
Î¸ and design matrix 
ğ‘‹
X, predictions are 
ğ‘¦
^
=
ğ‘‹
ğœƒ
y
^
	â€‹

=XÎ¸. The vectorized RMSE is:

RMSE
â€…â€Š
=
â€…â€Š
1
ğ‘š
â€‰
âˆ¥
ğ‘‹
ğœƒ
âˆ’
ğ‘¦
âˆ¥
2
2
RMSE=
m
1
	â€‹

âˆ¥XÎ¸âˆ’yâˆ¥
2
2
	â€‹

	â€‹


(where 
âˆ¥
ğ‘£
âˆ¥
2
2
=
ğ‘£
âŠ¤
ğ‘£
âˆ¥vâˆ¥
2
2
	â€‹

=v
âŠ¤
v is the sum of squared components of vector 
ğ‘£
v).

2) Intuition / interpretation

RMSE is the square root of the average squared prediction error.

Units: same as the target 
ğ‘¦
y (e.g., rupees if price is in rupees).

Sensitive to large errors (squares amplify big mistakes).

Lower RMSE = better average predictive accuracy (on the same scale as 
ğ‘¦
y).

3) Quick numeric example (using your house features)

Dataset (4 houses):

Area	Bedrooms	ACs	Price (actual 
ğ‘¦
y)
1200	3	2	5,000,000
1500	4	3	6,500,000
1000	2	1	3,500,000
1800	4	2	7,000,000

Suppose we pick parameter vector (weights):

â€…â€Š
ğœƒ
=
[
2000
,
â€…â€Š
50000
,
â€…â€Š
10000
]
âŠ¤
Î¸=[2000,50000,10000]
âŠ¤

(that is: â‚¹2000 per sq.ft, â‚¹50,000 per bedroom, â‚¹10,000 per AC).

Predictions 
ğ‘¦
^
=
ğ‘‹
ğœƒ
y
^
	â€‹

=XÎ¸ are:

House 1: 
1,200
â‹…
2000
+
3
â‹…
50000
+
2
â‹…
10000
=
2,570,000
1,200â‹…2000+3â‹…50000+2â‹…10000=2,570,000

House 2: 
1,500
â‹…
2000
+
4
â‹…
50000
+
3
â‹…
10000
=
3,230,000
1,500â‹…2000+4â‹…50000+3â‹…10000=3,230,000

House 3: 
1,000
â‹…
2000
+
2
â‹…
50000
+
1
â‹…
10000
=
2,110,000
1,000â‹…2000+2â‹…50000+1â‹…10000=2,110,000

House 4: 
1,800
â‹…
2000
+
4
â‹…
50000
+
2
â‹…
10000
=
3,820,000
1,800â‹…2000+4â‹…50000+2â‹…10000=3,820,000

Errors (prediction âˆ’ actual):

[
âˆ’
2,430,000
,
â€…â€Š
âˆ’
3,270,000
,
â€…â€Š
âˆ’
1,390,000
,
â€…â€Š
âˆ’
3,180,000
]
[âˆ’2,430,000,âˆ’3,270,000,âˆ’1,390,000,âˆ’3,180,000]

Square them, take mean, then square-root:

RMSE
â‰ˆ
2,675,925.07
RMSEâ‰ˆ2,675,925.07

So on average (in RMS sense) predictions are off by about â‚¹2.68 lakh Ã— 10 (i.e., ~â‚¹26.76 lakh) â€” or more plainly, ~â‚¹2.68 million â€” in the units of the price. That shows this 
ğœƒ
Î¸ is quite poor (just an arbitrary example).

4) Use in training

During training you typically minimize MSE (equivalently RMSE) with respect to 
ğœƒ
Î¸.

MSE is convenient because itâ€™s differentiable; gradient-based optimizers use 
âˆ‡
ğœƒ
MSE
âˆ‡
Î¸
	â€‹

MSE to update weights.

RMSE is just the square root of that; minimizing MSE also minimizes RMSE (monotonic relationship), so many algorithms optimize MSE directly.
