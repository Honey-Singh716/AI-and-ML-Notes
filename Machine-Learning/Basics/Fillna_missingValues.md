1. Import and create imputer
from sklearn.impute import SimpleImputer

my_imputer = SimpleImputer()


SimpleImputer() fills missing values in your dataset.

By default: strategy="mean" → replaces missing values with the mean of that column.

You can also use "median", "most_frequent", or a constant value.

2. Fit & transform training data
imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))


fit_transform does two steps:

fit() → learns the replacement value (e.g., mean) from X_train.

transform() → replaces missing values in X_train with those learned values.

After this step, imputed_X_train has no missing values.

3. Transform validation data
imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))


Important: we only call transform, not fit_transform, on validation/test data.

This ensures that missing values in validation data are filled using the same values learned from training data.

Prevents data leakage. ✅

4. Fix column names
imputed_X_train.columns = X_train.columns
imputed_X_valid.columns = X_valid.columns


When you do .fit_transform(), the result is a NumPy array (no column names).

Wrapping it with pd.DataFrame() loses the original feature names.

So we re-assign column names from the original dataset.

5. Model scoring
print("MAE from Approach 2 (Imputation):")
print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))


score_dataset is probably a helper function you wrote earlier.

It likely:

Fits a model on imputed_X_train, y_train.

Predicts on imputed_X_valid.

Computes MAE (Mean Absolute Error).

⚖️ Summary:

fit_transform(X_train) → learn imputation strategy + apply to training data.

transform(X_valid) → apply the same strategy to validation data (no leakage).

Reset column names since imputation strips them.

Evaluate model performance after imputation.
