# Randomness in Random Forest

Randomness in Random Forest is used **inside the algorithm itself**, not just in splitting train/test data.  
It plays a key role in making trees diverse and reducing overfitting.

---

## ğŸ”¹ Bootstrap Sampling (Bagging)

- For each tree, Random Forest doesnâ€™t use the entire training set.  
- Instead, it **randomly samples (with replacement)** from the training data to build each tree.  
- So Tree 1, Tree 2, Tree 3â€¦ all see different versions of the training set.  

âœ… This randomness ensures trees are **diverse**, not all identical.  

**Example:**  
- Suppose you have 1000 training points.  
- Each tree gets ~1000 samples, but drawn randomly.  
- Some points may repeat, some may be missing.  

This technique is called **Bagging (Bootstrap Aggregation)**.

---

## ğŸ”¹ Random Feature Selection (Feature Subsampling)

- When a tree splits a node, it **doesnâ€™t look at all features**.  
- It randomly selects a **subset of features**, then chooses the best split among those.  
- At the next split, another random subset is chosen.  

âœ… This prevents trees from always choosing the same strong features and increases diversity.  

**Example:**  
- Dataset has 20 features.  
- At each split, maybe only 5 random features are considered.  
- At the next split, a different 5 features might be chosen.

---

## ğŸ”‘ Why is Randomness Important?

- If all trees were trained on the **same data** with the **same features**,  
  â†’ theyâ€™d be identical.  
- The â€œforestâ€ would collapse into just **one big tree** (no benefit).  
- Randomness makes trees **different**, and when we **average/vote** their results:  
  - Variance is reduced.  
  - Generalization improves.  
  - Overfitting decreases.  

---

## âš™ï¸ Role of `random_state`

- Controls the **random seed** for:
  - Bootstrap sampling  
  - Random feature selection  
  - (Sometimes) shuffling  

- If fixed (`random_state=0`):  
  â†’ Reproducible results.  
- If not fixed (`random_state=None`):  
  â†’ Slightly different forests each run.  

---

## âœ… Summary

- **Random samples â†’ different trees (Bagging)**  
- **Random features â†’ different splits inside trees**  

Together they create **decorrelated trees**, and averaging them reduces variance â†’ better accuracy.  

---

## ğŸ“ Analogy

- **Random samples** â†’ Different students taking different practice questions.  
- **Random features** â†’ Each student solves problems using only some of the available formulas.  

When you combine their answers, you get a **strong overall solution**.
