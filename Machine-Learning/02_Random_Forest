# Randomness in Random Forest

Randomness in Random Forest is used **inside the algorithm itself**, not just in splitting train/test data.  
It plays a key role in making trees diverse and reducing overfitting.

---

## 🔹 Bootstrap Sampling (Bagging)

- For each tree, Random Forest doesn’t use the entire training set.  
- Instead, it **randomly samples (with replacement)** from the training data to build each tree.  
- So Tree 1, Tree 2, Tree 3… all see different versions of the training set.  

✅ This randomness ensures trees are **diverse**, not all identical.  

**Example:**  
- Suppose you have 1000 training points.  
- Each tree gets ~1000 samples, but drawn randomly.  
- Some points may repeat, some may be missing.  

This technique is called **Bagging (Bootstrap Aggregation)**.

---

## 🔹 Random Feature Selection (Feature Subsampling)

- When a tree splits a node, it **doesn’t look at all features**.  
- It randomly selects a **subset of features**, then chooses the best split among those.  
- At the next split, another random subset is chosen.  

✅ This prevents trees from always choosing the same strong features and increases diversity.  

**Example:**  
- Dataset has 20 features.  
- At each split, maybe only 5 random features are considered.  
- At the next split, a different 5 features might be chosen.

---

## 🔑 Why is Randomness Important?

- If all trees were trained on the **same data** with the **same features**,  
  → they’d be identical.  
- The “forest” would collapse into just **one big tree** (no benefit).  
- Randomness makes trees **different**, and when we **average/vote** their results:  
  - Variance is reduced.  
  - Generalization improves.  
  - Overfitting decreases.  

---

## ⚙️ Role of `random_state`

- Controls the **random seed** for:
  - Bootstrap sampling  
  - Random feature selection  
  - (Sometimes) shuffling  

- If fixed (`random_state=0`):  
  → Reproducible results.  
- If not fixed (`random_state=None`):  
  → Slightly different forests each run.  

---

## ✅ Summary

- **Random samples → different trees (Bagging)**  
- **Random features → different splits inside trees**  

Together they create **decorrelated trees**, and averaging them reduces variance → better accuracy.  

---

## 🎓 Analogy

- **Random samples** → Different students taking different practice questions.  
- **Random features** → Each student solves problems using only some of the available formulas.  

When you combine their answers, you get a **strong overall solution**.
